---
title: "NEID Solar Data -- Linear Modeling"
author: "Joe Salzer"
date: "`r Sys.Date()`"
output: html_document
---

This file is rstudio script for testing all of our modeling

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("readSpectra.R")
library(Matrix)
library(parallel)
library(pbmcapply)
```

# linear modeling

these will be passed to terminal
```{r}
covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_hg_coeff_", c(0,seq(2,10,1))))
#covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_thg_coeff_", c(0,seq(2,10,1))))
#covariateNames = c()

print(covariateNames)
```

Rscript rv_ols.R completeLines.csv
Rscript rv_irls.R completeLines.csv

Full:
fit_gauss_a,fit_gauss_b,fit_gauss_depth,fit_gauss_sigmasq,proj_hg_coeff_0,proj_hg_coeff_2,proj_hg_coeff_3,proj_hg_coeff_4,proj_hg_coeff_5,proj_hg_coeff_6,proj_hg_coeff_7,proj_hg_coeff_8,proj_hg_coeff_9,proj_hg_coeff_10

Gauss=all_HG=none:
fit_gauss_a,fit_gauss_b,fit_gauss_depth,fit_gauss_sigmasq

Gauss=none_HG=all:
proj_hg_coeff_0,proj_hg_coeff_2,proj_hg_coeff_3,proj_hg_coeff_4,proj_hg_coeff_5,proj_hg_coeff_6,proj_hg_coeff_7,proj_hg_coeff_8,proj_hg_coeff_9,proj_hg_coeff_10

```{r}
# get csv file name
csvFileName = "completeLines.csv"
```

## setting variables and loading data

variables to be set by the user

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# csv file name
completeLines_df = read_csv(str_c(WD_DATA,csvFileName)) %>%
  mutate(!!TIME_ID_NAME := as.Date( !!sym(TIME_ID_NAME) ))
```


## get list of lineIDs and timeIDs

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = completeLines_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = completeLines_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME))
T_ = length(timeIDs)
L_ = length(lineIDs)
```

## create model formula and subdirectories

the following chunk is the typical specification of the linear models as described in the paper and included in the script "rv_lm.R"

```{r}
# create directory called "models" in working directory
if (!dir.exists(str_c(WD_DATA,"models"))) {dir.create(str_c(WD_DATA,"models"))}

# base TWFE formula
twfe_formula = paste0("~ 0 ")

# if covariateNames aren't empty, include interactions, else use TWFE
if (!is_empty(covariateNames)) {
  
  # build interactions with LINE_ID_NAME for each provided centered covariate
  covar_slopes = paste0(LINE_ID_NAME,":", covariateNames, "_centered", collapse = " + ")
  # construct the full formula
  modelFormula = as.formula(paste(twfe_formula, covar_slopes, sep = " + "))
  
  # name the current model, using Gaussian fit parameters and hg covariateNames
  ## remove or edit below lines if using other covariates ##
  gaussCovars = covariateNames[startsWith(covariateNames,"fit_gauss")]
  gaussCovars_names = paste(str_split_i(gaussCovars, "_", i = 3),collapse=",")
  if (gaussCovars_names=="") {gaussCovars_names = "none"} else if(length(gaussCovars) == 4) {gaussCovars_names = "all"}
  hgCovars = covariateNames[startsWith(covariateNames,"proj_hg_coeff_")]
  hgCovars_names = paste(str_split_i(hgCovars, "_", i = 4),collapse=",")
  if (hgCovars_names=="") {hgCovars_names = "none"} else if(length(hgCovars) == 10) {hgCovars_names = "all"}
  model_name = str_c("Gauss=",gaussCovars_names,"_HG=",hgCovars_names)
  ## remove or edit above lines if using other covariates ##
  
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}

  rm(twfe_formula,covar_slopes,gaussCovars,gaussCovars_names,hgCovars,hgCovars_names)
} else {
  # if empty, just fit TWFE
  modelFormula = as.formula(twfe_formula)
  # model name
  model_name = "TWFE"
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}
  rm(twfe_formula)
}

cat("The model formula for the covariates is:\n", as.character(modelFormula), "\n")
cat("The model name is:", model_name, "\n")
```

below is the specification for the Common Slopes Model for covariateNames

*THG*
```{r}
model_name = "Gauss=all_THG=all"
# create model directory
model_dir = str_c(WD_DATA,"models/",model_name)
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

*COMMON SLOPES*
```{r}
modelFormula = as.formula(paste("~ 0 ", paste0(covariateNames, "_centered", collapse = " + "), sep = " + "))
# model name
model_name = "CommonSlopes"
# create model directory
model_dir = str_c(WD_DATA,"models/",model_name)
if (!dir.exists(model_dir)) {dir.create(model_dir)}

cat("The model formula for the covariates is:\n", as.character(modelFormula), "\n")
cat("The model name is:", model_name, "\n")
```

## standardize dataframe

center by lineID, scaling each continuous column. We can "inject" a planet here, by changing the amplitude, frequency, and horizontal offset 

*no planet*
```{r}
# get standardized rv arranging by line_order and then date
standardized_list = completeLines_df %>%
  standardize(covariates = covariateNames)

rv_df = standardized_list$train.df %>%
  arrange(LINE_ID_NAME, TIME_ID_NAME) %>%
  mutate(timeID = factor(!!sym(TIME_ID_NAME)),
         lineID = factor(!!sym(LINE_ID_NAME)),
         lineID_group_offset = factor(str_c(!!sym(LINE_ID_NAME),"_",!!sym(TIMEGROUP_ID_NAME))))

# set responses
responses = rv_df[[RESPONSE]]
```

*with planet*
```{r}
AMP = 3
HORIZONTAL_OFFSET = 10
VERTICAL_OFFSET = 0
FREQ = 2*pi/250

# get standardized rv, with an injected planet, arranging by line_order and then date
rv_df = ( injectPlanet(completeLines_df, amp = AMP, freq = FREQ, horizontal_offset = HORIZONTAL_OFFSET, vertical_offset = VERTICAL_OFFSET) %>%
  standardize(covariates = covariateNames ) )$train.df %>%
  arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME))

# mean offset for non-noisy planet signal, noisy planet signal, and noise (original rv signal)
# rv_df %>%
#   select(date,date_groups,pert_val,pert_rv,rv_template_0.5) %>%
#   unique() %>%
#   group_by(date_groups) %>%
#   summarize(planet_offset = mean(pert_val),
#             noisePlanet_offset = mean(pert_rv),
#             noise_offset = mean(rv_template_0.5))

# estimated_rv = rv_df %>%
#   group_by(date) %>%
#   summarize(est_rv = mean(pert_rv))

#seq_days = seq(min(estimated_rv$date), max(estimated_rv$date), by = 1)

# ggplot() +
#   geom_point(mapping = aes(x = estimated_rv$date, y = estimated_rv$est_rv)) +
#   geom_line(mapping = aes(x = seq_days,
#                           y = VERTICAL_OFFSET + AMP*sin(FREQ*changeFun(as.numeric(seq_days))+HORIZONTAL_OFFSET) ) )

# set responses
responses = rv_df$planet_rv

rm(AMP,HORIZONTAL_OFFSET,VERTICAL_OFFSET,FREQ,estimated_rv,seq_days)
```

```{r}
trueRV = rv_df %>%
  group_by(date) %>%
  summarize(trueRV = mean(planet_val),
         date_groups = mean(date_groups)) %>%
  ungroup() %>%
  group_by(date_groups) %>%
  mutate(trueRV_group = trueRV - mean(trueRV)) %>%
  ungroup() %>%
  pull(trueRV_group)
head(trueRV)
```

## set contrasts and make design matrix

*group sum2zero for timeIDs, sum2zero for lineIDs (single intercept, single slope model)*

```{r}
# get group sizes
group_sizes = rv_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
```

```{r}
# time fixed effects encoding matrix
S_time = cbind(
  bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
  bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
# time fixed effects design matrix
X_time = kronecker(rep(1,L_), S_time)
colnames(X_time) = str_c("alpha",1:ncol(X_time))
dim(X_time)
```

```{r}
# design matrix for the line fixed effects
X_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_) )
colnames(X_line) = str_c("beta",1:ncol(X_line))
dim(X_line)
```

```{r}
# design matrix for the covariates
X_covar = sparse.model.matrix(modelFormula,rv_df) 
dim(X_covar)
```

```{r}
# create full design matrix
designMat = cbind(rep(1,L_*T_),X_time,X_line,X_covar)
dim(designMat)

rm(X_time,X_line,X_covar)
```

```{r}
T_+L_+L_*14-1
T_+L_+L_*0-1
```

*group sum2zero for timeIDs, sum2zero for lineIDs (multiple intercept, single slope model)*

```{r}
# get group sizes
group_sizes = rv_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
# get encoding for time effects offset
S_time = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
# time fixed effects design matrix
X_time = kronecker(rep(1,L_), S_time)
dim(X_time)
```

```{r}
X_line = sparse.model.matrix(~0 +lineID_group_offset + line_order:fit_gauss_a_centered + line_order:fit_gauss_b_centered + 
    line_order:fit_gauss_depth_centered + line_order:fit_gauss_sigmasq_centered + 
    line_order:proj_hg_coeff_0_centered + line_order:proj_hg_coeff_2_centered + 
    line_order:proj_hg_coeff_3_centered + line_order:proj_hg_coeff_4_centered + 
    line_order:proj_hg_coeff_5_centered + line_order:proj_hg_coeff_6_centered + 
    line_order:proj_hg_coeff_7_centered + line_order:proj_hg_coeff_8_centered + 
    line_order:proj_hg_coeff_9_centered + line_order:proj_hg_coeff_10_centered, rv_df,
                             contrasts.arg = list(lineID_group_offset = contr.sum,
                                                  line_order = contr.sum))
dim(X_line)
```

```{r}
# create full design matrix
designMat = cbind(X_time,X_line)
dim(designMat)
```
```{r}
T_+2*L_+L_*14-2
T_+2*L_+L_*0-2
```

*group sum2zero for timeIDs, sum2zero for lineIDs (multiple intercept, multiple slope model)*

```{r}
# get group sizes
group_sizes = rv_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
# get encoding for time effects offset
S_time = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
# time fixed effects design matrix
X_time = kronecker(rep(1,L_), S_time)
dim(X_time)
```

```{r}
X_line = sparse.model.matrix(~0 +lineID_group_offset + lineID_group_offset:fit_gauss_a_centered + lineID_group_offset:fit_gauss_b_centered + 
    lineID_group_offset:fit_gauss_depth_centered + lineID_group_offset:fit_gauss_sigmasq_centered + 
    lineID_group_offset:proj_hg_coeff_0_centered + lineID_group_offset:proj_hg_coeff_2_centered + 
    lineID_group_offset:proj_hg_coeff_3_centered + lineID_group_offset:proj_hg_coeff_4_centered + 
    lineID_group_offset:proj_hg_coeff_5_centered + lineID_group_offset:proj_hg_coeff_6_centered + 
    lineID_group_offset:proj_hg_coeff_7_centered + lineID_group_offset:proj_hg_coeff_8_centered + 
    lineID_group_offset:proj_hg_coeff_9_centered + lineID_group_offset:proj_hg_coeff_10_centered, rv_df,
                             contrasts.arg = list(lineID_group_offset = contr.sum))
dim(X_line)
```
```{r}
# create full design matrix
designMat = cbind(X_time,X_line)
dim(designMat)
```
```{r}
T_+2*L_+L_*14*2-2
T_+2*L_+L_*0-2
```


*NOTATION FOR THE MISS model, line_group fixed effects*

```{r}
L_ = 3
g = c(3, 2)  # Can be any vector

# number of groups
G_ = length(g)

# Sum-to-zero contrast matrix
S = contr.sum(L_ * G_)

# Construct block diagonal matrix dynamically
R = kronecker(diag(L_), bdiag(lapply(g, function(n) rep(1, n))))

R
# Compute X
X = R %*% S
X
```


## fit model

```{r}
# fit the linear model using all data
fit_lm = sparseLM(designMat, responses)
```

## get cleaned RVs

*single intercept, single slope*
```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = ncol(designMat), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
```
*multiple intercept*
```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = ncol(designMat), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,1:(sum(group_sizes)-2)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
```


```{r}
# covariance matrix of model parameters
cov_mat = fit_lm$var_beta_hat

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
cleanRV_covar = linear_op_mat %*% cov_mat %*% t(linear_op_mat)
cleanRV_df$var = diag( cleanRV_covar )
cleanRV_df$se = sqrt(cleanRV_df$var)
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```



```{r}
cleanRV_df %>%
  ggplot() +
  geom_point(mapping = aes(x = timeID, y = estimate))
```

## save fit

```{r}
saveRDS(list(designMat = designMat,
             responses = responses,
             df = completeLines_df,
             group_sizes = group_sizes,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE),
        file = str_c(model_dir, "/model.rds" ) )
```

# leverage calculations

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "Gauss=all_HG=all"
BATCH_SIZES = 1000
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
fit_lm = model_fit$fit_lm
X = model_fit$designMat
XtX_inv = (fit_lm$var_beta_hat)/fit_lm$sigma2_hat
```

```{r}
# directory containing the leverage files
leverage_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/leverages")
# create directory called "leverages" in model name
if ( !dir.exists(leverage_dir) ) {
  dir.create(leverage_dir)
}
```

```{r}
# optimized leverage computation
compute_leverage_batches = function(indices) {
  start_row = indices[1]
  end_row = indices[2]
  
  # extract rows in a batch
  rows = X[start_row:end_row, , drop = FALSE]
  # compute leverage for all rows in the batch
  leverage = rowSums(rows * (rows %*% XtX_inv))
  
  names(leverage) = str_c("row",start_row:end_row)

  # save results in a single file for the batch
  batch_filename = file.path(leverage_dir, str_c("leverage_", start_row, "_", end_row, ".rds"))
  saveRDS(leverage, file = batch_filename)
}

# function to create batches
create_batches = function(n_rows, batch_size) {
  # start indices of batches
  starts = seq(1, n_rows, by = batch_size)
  # end indices of batches
  ends = pmin(starts + batch_size - 1, n_rows)
  # combine into a list of start and end indices
  batches = Map(c, starts, ends)
  return(batches)
}
```

```{r}
# create batch indices
batches = create_batches(nrow(X), BATCH_SIZES)
```

```{r}
# test
compute_leverage_batches(batches[[1]])
```

```{r}
# parallel computation of leverage
result <- pbmclapply(batches, FUN = compute_leverage_batches, mc.cores = 7)
```

```{r}
# get the list of all .rds files in the leverage directory
leverage_files = list.files(path = leverage_dir, pattern = "\\.rds$", full.names = TRUE)
# initialize an empty vector to store the combined leverage values
leverages = vector()
for (file in leverage_files) {
  leverages = c(leverages, readRDS(file))
}
rm(leverage_files)
```

```{r}
head(leverages)
length(leverages)
```

```{r}
saveRDS(leverages,
        str_c(WD_DATA, "models/", MODEL_NAME, "/leverages.rds" ))
```

# bootstrap coefficients and residuals

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "Gauss=all_HG=all"
NUM_BOOTS = 2500
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
```

```{r}
# directory containing the bootstrap files
bootstrap_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/wild_bootstraps" )
# create directory called "bootstrap" in model name
if ( !dir.exists(bootstrap_dir) ) {
  dir.create(bootstrap_dir)
}
```

```{r}
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
# lm fit, df, design matrix and leverages
fit_lm = model_fit$fit_lm
rv_df = model_fit$df
designMat = model_fit$designMat
leverages = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/leverages.rds" ))

# vec of LineIDs  in completeLines_df
lineIDs = rv_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(rv_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```


```{r}
# get modified residuals
u_hat = fit_lm$resid/(1-leverages)
```

assign blocks for the bootstrap
```{r}
# assign blocks for the bootstrap (by line)
block_size = length(timeIDs)

rv_df = rv_df %>%
  arrange(!!sym(LINE_ID_NAME),!!sym(TIME_ID_NAME)) %>%
  mutate(boot_block = factor( ceiling(row_number() / block_size) ) )
```

```{r}
wildBoot_parallel = function(seedID) {
  
  # set a given seed for replication
  set.seed(seedID)
  
  # ensure that the rv_df is arranged first by lineID and then timeID, and that these are factors
  rv_df = rv_df %>%
    arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    mutate( lineID = factor(!!sym(LINE_ID_NAME)),
            timeID = factor(!!sym(TIME_ID_NAME)))
  
  # rademacher random variable for each block in the dataset
  v = sample( c(-1, 1), size = length(lineIDs), replace = TRUE)
  # generate wild bootstrap residuals by block
  u_star = u_hat*v[rv_df$boot_block]
  
  # construct bootstrap sample
  y_star = fit_lm$y_hat + u_star
  # re-estimate the model on bootstrap sample
  fitstar_lm = sparseLM(designMat, y_star, PRINT_TIME = F)

  # store data
  saveRDS(fitstar_lm$beta_hat[,1],
          file = str_c(bootstrap_dir, "/bootstraps_list_", seedID, ".rds" ) )
  
}
```

```{r}
wildBoot_parallel(1)
```

run wild bootstrap

```{r}
boot_straps <- pbmclapply(100*( c(1:NUM_BOOTS) + 2000 ), FUN = wildBoot_parallel, mc.cores = 7)
rm(boot_straps)
```

# cross validation

```{r}
# pass on command line

# model name
MODEL_NAME = "Gauss=All_HG=all"
# number of timeIDs in each leave-one-out cv
CV_NUM_DAY = 150
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds" ))
df = model_fit$df
covariateNames = model_fit$covariateNames
modelFormula = model_fit$modelFormula
```

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```

```{r}
# create directory called "cv" in working directory
if ( !dir.exists(str_c(WD_DATA,"models/",MODEL_NAME,"/cv_block=",2*CV_NUM_DAY)) ) {
  dir.create(str_c(WD_DATA,"models/",MODEL_NAME,"/cv_block=",2*CV_NUM_DAY))
}
```

```{r}
# list of timeIDs to be left out
left_out_timeIDs = create_sliding_windows(timeIDs, CV_NUM_DAY)
left_out_timeIDs[[1]]
left_out_timeIDs[[2]]
left_out_timeIDs[[15]]
left_out_timeIDs[[227]]
left_out_timeIDs[[228]]
```

```{r}
# length of left-out dataset
summary( sapply(left_out_timeIDs, FUN = function(x) length(x$test_set) ) )
# length of left-in dataset
summary( T_-sapply(left_out_timeIDs, FUN = function(x) length(x$test_set) ) )
```

```{r}
cv_parallel = function(i) {

  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
  
  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  # get group sizes
  group_sizes_train = train_df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  group_sizes = df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  
  # time fixed effects encoding matrix
  if (length(group_sizes_train) > 1) {
    S_time = cbind(
      bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
      bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
    )
  } else {
    S_time = contr.sum(group_sizes_train)
  }
  # time fixed effects design matrix
  X_train_time = kronecker(rep(1,L_), S_time)
  # design matrix for the line fixed effects
  X_train_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_-length(test_set)) )
  # design matrix for the covariates
  X_train_covar = sparse.model.matrix(modelFormula,train_df) 
  # create full design matrix
  X_train = cbind(rep(1,nrow(X_train_time)),X_train_time,X_train_line,X_train_covar)
  
  rm(X_train_time, X_train_line, X_train_covar)
  
  # responses
  Y_train = train_df[[RESPONSE]]
    
  # fit the linear model using all data
  fit_lm = sparseLM(X_train, Y_train)
  
  # get the group-offset encoding for the test set test set based on time point
  g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[(timeIDs %in% test_set),]
  S_test_time = cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
  X_test_time = kronecker(rep(1,L_), S_test_time)
  
  # design matrix for the line fixed effects
  X_test_line = kronecker( contr.sum(L_, sparse = T), rep(1,length(test_set)) )
  # design matrix for the covariates
  X_test_covar = sparse.model.matrix(modelFormula,test_df) 
  # create full design matrix
  X_test = cbind(rep(1,nrow(X_test_covar)),X_test_time,X_test_line,X_test_covar)
  # responses
  Y_test = test_df[[RESPONSE]]
  
  rm(X_test_time, X_test_line, X_test_covar)
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% fit_lm$beta_hat)[,1]


  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = fit_lm$beta_hat[,1]),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv_block=",2*CV_NUM_DAY,"/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel, mc.cores = 8)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```
*LASSO*
```{r}
cv_parallel_LASSO = function(i) {

  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
  
  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  # get group sizes
  group_sizes_train = train_df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  group_sizes = df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  
  # time fixed effects encoding matrix
  if (length(group_sizes_train) > 1) {
    S_time = cbind(
      bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
      bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
    )
  } else {
    S_time = contr.sum(group_sizes_train)
  }
  # time fixed effects design matrix
  X_train_time = kronecker(rep(1,L_), S_time)
  # design matrix for the line fixed effects
  X_train_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_-length(test_set)) )
  # design matrix for the covariates
  X_train_covar = sparse.model.matrix(modelFormula,train_df) 
  # create full design matrix
  X_train = cbind(rep(1,nrow(X_train_time)),X_train_time,X_train_line,X_train_covar)
  
  # remove LASSO columns
  X_train = X_train[, !(colnames(X_train) %in% model_fit$lasso_zero_columns)]
  
  rm(X_train_time, X_train_line, X_train_covar)
  
  # responses
  Y_train = train_df[[RESPONSE]]
    
  # fit the linear model using all data
  fit_lm = sparseLM(X_train, Y_train)
  
  # get the group-offset encoding for the test set test set based on time point
  g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[(timeIDs %in% test_set),]
  S_test_time = cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
  X_test_time = kronecker(rep(1,L_), S_test_time)
  
  # design matrix for the line fixed effects
  X_test_line = kronecker( contr.sum(L_, sparse = T), rep(1,length(test_set)) )
  # design matrix for the covariates
  X_test_covar = sparse.model.matrix(modelFormula,test_df) 
  # create full design matrix
  X_test = cbind(rep(1,nrow(X_test_covar)),X_test_time,X_test_line,X_test_covar)
  # responses
  Y_test = test_df[[RESPONSE]]
  
  # remove LASSO columns
  X_test = X_test[, !(colnames(X_test) %in% model_fit$lasso_zero_columns)]
  
  rm(X_test_time, X_test_line, X_test_covar)
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% fit_lm$beta_hat)[,1]


  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = fit_lm$beta_hat[,1]),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv_block=",2*CV_NUM_DAY,"/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_parallel_LASSO(left_out_timeIDs[[1]])
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel_LASSO, mc.cores = 6)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```

*playground*
```{r}
group_sizes_train = c(4,3,4)
if (length(group_sizes_train) > 1) {
  S_time_train = cbind(
    bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
    bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
  )
} else {
  S_time_train = contr.sum(group_sizes_train)
}
S_time_train

group_sizes = c(5,4,4)
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```

```{r}
test_set = c(5:6)
g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[test_set,]
g_t

cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
```


```{r}
group_sizes = c(5,4,3)
bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes))
# time fixed effects encoding matrix
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```

```{r}
group_sizes = c(5,3,3)
bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes))
# time fixed effects encoding matrix
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```



*testing CV function*

```{r}
i = left_out_timeIDs[[227]]

# get the test set and which day we evaluate on
test_set = i$test_set
test_day = i$test_day

# test index if we leave out a given day/several timeIDs
cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  

# get standardized rv dataframe, seperate into train and test datasets
standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
```

```{r}
# get training and testing sets
train_df = standardized_list$train.df %>%
  mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
         !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
  arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

test_df = standardized_list$test.df %>%
  mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
         !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
  arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

# get group sizes
group_sizes_train = train_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
group_sizes = df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
```

get train design mat and fit model
```{r}
# time fixed effects encoding matrix
if (length(group_sizes_train) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
    bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes_train)
}
# time fixed effects design matrix
X_train_time = kronecker(rep(1,L_), S_time)
# design matrix for the line fixed effects
X_train_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_-length(test_set)) )
# design matrix for the covariates
X_train_covar = sparse.model.matrix(modelFormula,train_df) 
# create full design matrix
X_train = cbind(rep(1,nrow(X_train_time)),X_train_time,X_train_line,X_train_covar)

rm(X_train_time, X_train_line, X_train_covar)

# responses
Y_train = train_df[[RESPONSE]]
```

```{r}
# fit the linear model using all data
fit_lm = sparseLM(X_train, Y_train)
```


```{r}
# get the group-offset encoding for the test set test set based on time point
g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[(timeIDs %in% test_set),]
S_test_time = cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
X_test_time = kronecker(rep(1,L_), S_test_time)

# design matrix for the line fixed effects
X_test_line = kronecker( contr.sum(L_, sparse = T), rep(1,length(test_set)) )
# design matrix for the covariates
X_test_covar = sparse.model.matrix(modelFormula,test_df) 
# create full design matrix
X_test = cbind(rep(1,nrow(X_test_covar)),X_test_time,X_test_line,X_test_covar)
# responses
Y_test = test_df[[RESPONSE]]

rm(X_test_time, X_test_line, X_test_covar)

# create a column for predicted rvs
test_df[["pred_rv"]] = (X_test %*% fit_lm$beta_hat)[,1]
```

```{r}
test_df %>%
  rename(contam_rv = !!sym(RESPONSE) ) %>%
  select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv) %>%
  summarize(rmse = sqrt(mean((contam_rv-pred_rv)^2)) )
```

# Iteratvely Reweighted least squares

*run the full IRLS procedure*
```{r}
###########################
## fit a LM using model matrix and responses##
###########################

# rse_tol : tolerance for the RSE as it changes
# max.iter : maximum number of iterations to run
# X (n x p) : model matrix
# Y (n x 1) : responses
# df : dataframe of observations
# lineIDname : column name of the lines

IRLS = function(rse_tol = 2e-05,
                max.iter = 10,
                X = designMat,
                Y = responses,
                df = completeLines_df,
                lineIDname = LINE_ID_NAME) {
  ## initialize equal weights ##
  iter = 1
  # weight vector
  w_irls = rep(1, nrow(X))
  wls_fit = sparseWLM(X, Y, w_irls)
  # rse for each iteration
  rse_iter = list()
  rse_iter[[1]] = 0
  rse_iter[[2]] = wls_fit$RSE
  
  while( (iter < max.iter) & (rse_iter[[iter+1]] - rse_iter[[iter]]>rse_tol) ) {
    iter = iter + 1
    # re-weight line-by-line, update weight vector
    w_irls = df %>%
      mutate(r = wls_fit$resid[,1]) %>%
      group_by(!!sym(lineIDname)) %>%
      mutate(sar = sum(abs(r)),
             w = 1/sar) %>%
      ungroup() %>%
      pull(w)
    w_irls = w_irls/sum(w_irls)
    # fit new model
    wls_fit = sparseWLM(X, Y, w_irls)
    rse_iter[[iter + 1]] = wls_fit$RSE
    #print(iter)
  }
  cat("iter: ", iter, " diff RSE: ", rse_iter[[iter+1]] - rse_iter[[iter]], "\n")
  return(list(fit_lm = wls_fit,
              weights = w_irls) )
}
```

```{r}
irls_results = IRLS(max.iter = 20)
fit_lm = irls_results$fit_lm
w_irls = irls_results$weights
```

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = ncol(designMat), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```

```{r}
if (!dir.exists(str_c(model_dir,"_IRLS"))) {dir.create(str_c(model_dir,"_IRLS"))}

saveRDS(list(designMat = designMat,
             responses = responses,
             df = completeLines_df,
             group_sizes = group_sizes,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE),
        file = str_c(model_dir, "_IRLS/model.rds" ) )
```



End result

*Baseline Model -- OLS*
AIC: 17.06112 x 10^5 
BIC: 17.17686 x 10^5 
RMSE: 1.91929 
RSE: 27.61435
Effective sample size: 256740 
*Full Model -- OLS*
AIC: 9.938 x 10^5 
BIC: 11.192 x 10^5
RMSE: 0.575
RSE: 6.610317
Effective sample size: 256740
*Baseline Model -- IRLS*
AIC: 17.06713 x 10^5 
BIC: 17.18288 x 10^5 
RMSE: 1.722021 
RSE: 27.64671 
Effective sample size: 179739.2 
*Full Model -- IRLS*
AIC: 9.945407 x 10^5 
BIC: 11.2 x 10^5 
RMSE: 0.4032205 
RSE: 6.620226 
Effective sample size: 172722.2 




# LASSO feature selection

```{r}
library(glmnet)
```

change model name if necessary
```{r}
model_name = "Gauss=all_HG=all_IRLS"

model_dir = str_c(WD_DATA,"models/LASSO_IRLS")
# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

```{r}
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", model_name, "/model.rds"))
# lm fit, df, design matrix and leverages
fit_lm = model_fit$fit_lm
X = model_fit$designMat
y = model_fit$responses
model_weights = model_fit$weights
```

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = ncol(designMat), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
```

```{r}
# set the penalty factors
penalty_factors = rep(1, ncol(X[,-1]))

# determine number of parameters to not penalize (time FE)
# SISS
non_penalized = (T_-1)+(L_-1)
# MIMS - modify this
# non_penalized = (T_-2)

# set the non-penalized columns
penalty_factors[1:non_penalized] = 0
```

```{r}
# lasso fit
lasso_fit = glmnet(X[,-1], y,
                   alpha = 1,
                   penalty.factor = penalty_factors,
                   weights = model_weights,
                   intercept = T,
                   lambda = 0.0004)

lasso_coef = coef(lasso_fit)
cleanRV_LASSO = (linear_op_mat %*% lasso_coef)[,1]


# percent columns removed
print( 100*mean( (lasso_coef == 0)[,1] ) )
# rmse of LASSO
print( sqrt( mean(cleanRV_LASSO^2) ) )


# get coefs that are exactly 0 in LASSO
zero_columns = (lasso_coef==0)[,1]
# remove those columns from the design matrix
designMat_LASSO = X[,!zero_columns]
# fit the linear model using all data
fit_lm = sparseWLM(designMat_LASSO, y, model_weights)

gamma = fit_lm$beta_hat
alpha = gamma[3:T_]
cleanRV = bdiag(lapply(group_sizes, function(n) contr.sum(n) )) %*% alpha

# store results
fit_lm$AIC/1e5
fit_lm$BIC/1e5
sqrt( mean(( cleanRV )^2) )
fit_lm$RSE
```




lambda = 0.002
[1] 33.68614
[1] 1.494275
Time difference of 8.660292 secs
[1] 10.73447
[1] 11.56644
[1] 1.122673
[1] 7.966794

lambda = 0.0007
[1] 21.89349
[1] 1.142513
Time difference of 12.1647 secs
[1] 10.24715
[1] 11.22707
[1] 0.7070691
[1] 7.226224

lambda = 0.0004
[1] 15.03459
[1] 0.9779212
Time difference of 14.03362 secs
[1] 10.07808
[1] 11.14405
[1] 0.5449326
[1] 6.98142

lambda = 0.0003
[1] 11.88432
[1] 0.9122636
Time difference of 15.34105 secs
[1] 10.02787
[1] 11.13337
[1] 0.4956443
[1] 6.908614

lambda = 0.0002
[1] 8.784065
[1] 0.8567977
Time difference of 16.4001 secs
[1] 9.992543
[1] 11.13693
[1] 0.4550395
[1] 6.85649

lambda = 0.00005
[1] 1.791816
[1] 0.7432322
Time difference of 18.66662 secs
[1] 9.944159
[1] 11.17627
[1] 0.4072171
[1] 6.78161

lambda = 0.00002
[1] 0.2250188
[1] 0.7483954
Time difference of 18.81867 secs
[1] 9.945312
[1] 11.19708
[1] 0.4033595
[1] 6.780771


MIMS
```{r}
# get coefs that are exactly 0 in LASSO
zero_columns = (lasso_coef[-1]==0)
100*sum(zero_columns)/nrow(X)
# remove those columns from the design matrix
designMat_LASSO = X[,!zero_columns]
# fit the linear model using all data
fit_lm = sparseLM(designMat_LASSO, y)

fit_lm$RSE
fit_lm$AIC/1e5

fit_lm$RSE
fit_lm$AIC/1e5
```


```{r}
# initialize runs, lambdas, number of zero columns, AIC, BIC, and RMSE

num_runs = 50
lambdas = seq(0, 0.0004, length.out = num_runs)

numZeroCols = rep(NA,num_runs)
AICs = rep(NA,num_runs)
BICs = rep(NA,num_runs)
RMSEs = rep(NA,num_runs)
RSEs = rep(NA,num_runs)
```


```{r}
for (i in 1:length(lambdas)) {
  # set a lambda for our regularizer
  lam = lambdas[i]
  # lasso fit
  lasso_fit = glmnet(X[,-1], y,
                     alpha = 1,
                     penalty.factor = penalty_factors,
                     weights = model_weights,
                     intercept = T,
                     lambda = lam)
  
  lasso_coef = coef(lasso_fit)
  cleanRV_LASSO = (linear_op_mat %*% lasso_coef)[,1]
  
  # get coefs that are exactly 0 in LASSO
  zero_columns = (lasso_coef==0)[,1]
  # remove those columns from the design matrix
  designMat_LASSO = X[,!zero_columns]
  # fit the linear model using all data
  fit_lm = sparseWLM(designMat_LASSO, y, model_weights)
  
  gamma = fit_lm$beta_hat
  alpha = gamma[3:T_]
  cleanRV = bdiag(lapply(group_sizes, function(n) contr.sum(n) )) %*% alpha
  
  # store results
  AICs[i] = fit_lm$AIC
  BICs[i] = fit_lm$BIC
  RMSEs[i] = sqrt( mean(( cleanRV )^2) )
  RSEs[i] = fit_lm$RSE
  numZeroCols[i] = sum(zero_columns)
}
```

```{r}
saveRDS(
  list(AICs = AICs, 
       BICs = BICs, 
       RMSEs = RMSEs,
       RSEs = RSEs,
       lambdas = lambdas,
       numZeroCols = numZeroCols),
  str_c(model_dir, "/LASSO_results.rds"))
```


old
```{r}
# for (i in 1:length(lambdas)) {
#   # set a lambda for our regularizer
#   lam = lambdas[i]
#   # lasso coefficients
#   lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = lam, penalty.factor = penalty_factors) )[,1]
#   
#   # get coefs that are exactly 0 in LASSO
#   zero_columns = (lasso_coef==0)
# 
#   
#   # remove those columns from the design matrix
#   designMat_LASSO = X[,!zero_columns]
#   # fit the linear model using all data
#   fit_lm = sparseLM(designMat_LASSO, y)
# 
#   gamma = fit_lm$beta_hat[,1]
#   alpha = gamma[3:330]
#   cleanRV = (bdiag(lapply(group_sizes, contr.sum)) %*% alpha)[,1]
#   
#   # store results
#   AICs[i] = fit_lm$AIC
#   BICs[i] = fit_lm$BIC
#   RMSEs[i] = sqrt( mean(( cleanRV )^2) )
#   RSEs[i] = fit_lm$RSE
#   numZeroCols[i] = sum(zero_columns)
# }
```

```{r}
saveRDS(
  list(AICs = AICs, 
       BICs = BICs, 
       RMSEs = RMSEs,
       RSEs = RSEs,
       lambdas = lambdas,
       numZeroCols = numZeroCols),
  str_c(model_dir, "/LASSO_results.rds"))
```

## view LASSO results

```{r}
lasso_results = readRDS(str_c(model_dir, "/LASSO_results.rds"))
#lasso_results
```

```{r}
ggplot() +
  geom_point(mapping = aes(x = lasso_results$numZeroCols, lasso_results$AICs)) +
  xlab("percent of columns removed") +
  ylab("AIC") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$BICs)) +
  xlab("percent of columns removed") +
  ylab("BIC") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$RMSEs)) +
  xlab("percent of columns removed") +
  ylab("RMSE") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$RSEs)) +
  xlab("percent of columns removed") +
  ylab("RSE")  +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)]) +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
```


```{r}
# full model (no lasso) RMSE
lasso_results$RMSEs[1]
```

```{r}
# RMSE differences between no LASSO vs min AIC LASSO
lasso_results$RMSEs[which.min(lasso_results$AICs)]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$AICs)]
```

```{r}
# RMSE differences between no LASSO vs min BIC LASSO
lasso_results$RMSEs[which.min(lasso_results$BICs)]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$BICs)]
```


*fitting a linear model with the LASSO chosen features*

```{r}
# set a lambda for our regularizer via AIC
#lam = lasso_results$lambdas[which.min(lasso_results$AICs)]

# set a lambda for our regularizer via BIC
lam = lasso_results$lambdas[which.min(lasso_results$BICs)]
```


```{r}
# fit lasso
lasso_fit = glmnet(X[,-1], y,
                   alpha = 1,
                   penalty.factor = penalty_factors,
                   weights = model_weights,
                   intercept = T,
                   lambda = lam)
# get coefs from a LASSO
lasso_coef = coef(lasso_fit)
# get coefs that are exactly 0 in LASSO
zero_columns = (lasso_coef==0)[,1]
# remove those columns from the design matrix
designMat_LASSO = X[,!zero_columns]
# fit the linear model using all data
fit_lm = sparseWLM(designMat_LASSO, y, model_weights)
```


```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = ncol(designMat_LASSO), sparse = T ) 
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat)[,1]
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```

```{r}
saveRDS(list(designMat = designMat_LASSO,
             responses = responses,
             df = rv_df,
             group_sizes = group_sizes,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE,
             lasso_zero_columns = zero_columns),
        file = str_c(model_dir, "/model_BIC.rds" ) )
```

